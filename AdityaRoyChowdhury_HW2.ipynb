{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWQPke0BxDoC",
        "outputId": "31dc868a-194d-4dbc-c9f2-b8f58957c89b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to C:\\Users\\Aditya\n",
            "[nltk_data]     Roy\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import Perceptron, LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "import gensim.downloader as api\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn.functional as F\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOpVZ4VgxDoH"
      },
      "source": [
        "## Read Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WctlhZf-xDoI",
        "outputId": "524f9089-1589-44e8-8aab-760f4031e8ca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Aditya Roy\\AppData\\Local\\Temp\\ipykernel_10004\\3419564228.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  data = pd.read_csv(r'C:\\Users\\Aditya Roy\\Downloads\\Data\\Dataset.tsv', sep=r\"\\t\")\n"
          ]
        }
      ],
      "source": [
        "#Absolute Path to dataset\n",
        "\n",
        "data = pd.read_csv(r'C:\\Users\\Aditya Roy\\Downloads\\Data\\Dataset.tsv', sep=r\"\\t\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yXTFCRsxDoJ",
        "outputId": "53f61448-a82a-4e63-8f47-2e5e7dd634fb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>marketplace</th>\n",
              "      <th>customer_id</th>\n",
              "      <th>review_id</th>\n",
              "      <th>product_id</th>\n",
              "      <th>product_parent</th>\n",
              "      <th>product_title</th>\n",
              "      <th>product_category</th>\n",
              "      <th>star_rating</th>\n",
              "      <th>helpful_votes</th>\n",
              "      <th>total_votes</th>\n",
              "      <th>vine</th>\n",
              "      <th>verified_purchase</th>\n",
              "      <th>review_headline</th>\n",
              "      <th>review_body</th>\n",
              "      <th>review_date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>US</td>\n",
              "      <td>43081963</td>\n",
              "      <td>R18RVCKGH1SSI9</td>\n",
              "      <td>B001BM2MAC</td>\n",
              "      <td>307809868</td>\n",
              "      <td>Scotch Cushion Wrap 7961, 12 Inches x 100 Feet</td>\n",
              "      <td>Office Products</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Five Stars</td>\n",
              "      <td>Great product.</td>\n",
              "      <td>2015-08-31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>US</td>\n",
              "      <td>10951564</td>\n",
              "      <td>R3L4L6LW1PUOFY</td>\n",
              "      <td>B00DZYEXPQ</td>\n",
              "      <td>75004341</td>\n",
              "      <td>Dust-Off Compressed Gas Duster, Pack of 4</td>\n",
              "      <td>Office Products</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Phffffffft, Phfffffft. Lots of air, and it's C...</td>\n",
              "      <td>What's to say about this commodity item except...</td>\n",
              "      <td>2015-08-31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>US</td>\n",
              "      <td>21143145</td>\n",
              "      <td>R2J8AWXWTDX2TF</td>\n",
              "      <td>B00RTMUHDW</td>\n",
              "      <td>529689027</td>\n",
              "      <td>Amram Tagger Standard Tag Attaching Tagging Gu...</td>\n",
              "      <td>Office Products</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>but I am sure I will like it.</td>\n",
              "      <td>Haven't used yet, but I am sure I will like it.</td>\n",
              "      <td>2015-08-31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>US</td>\n",
              "      <td>52782374</td>\n",
              "      <td>R1PR37BR7G3M6A</td>\n",
              "      <td>B00D7H8XB6</td>\n",
              "      <td>868449945</td>\n",
              "      <td>AmazonBasics 12-Sheet High-Security Micro-Cut ...</td>\n",
              "      <td>Office Products</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>and the shredder was dirty and the bin was par...</td>\n",
              "      <td>Although this was labeled as &amp;#34;new&amp;#34; the...</td>\n",
              "      <td>2015-08-31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>US</td>\n",
              "      <td>24045652</td>\n",
              "      <td>R3BDDDZMZBZDPU</td>\n",
              "      <td>B001XCWP34</td>\n",
              "      <td>33521401</td>\n",
              "      <td>Derwent Colored Pencils, Inktense Ink Pencils,...</td>\n",
              "      <td>Office Products</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>Y</td>\n",
              "      <td>Four Stars</td>\n",
              "      <td>Gorgeous colors and easy to use</td>\n",
              "      <td>2015-08-31</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
              "0          US     43081963  R18RVCKGH1SSI9  B001BM2MAC       307809868   \n",
              "1          US     10951564  R3L4L6LW1PUOFY  B00DZYEXPQ        75004341   \n",
              "2          US     21143145  R2J8AWXWTDX2TF  B00RTMUHDW       529689027   \n",
              "3          US     52782374  R1PR37BR7G3M6A  B00D7H8XB6       868449945   \n",
              "4          US     24045652  R3BDDDZMZBZDPU  B001XCWP34        33521401   \n",
              "\n",
              "                                       product_title product_category  \\\n",
              "0     Scotch Cushion Wrap 7961, 12 Inches x 100 Feet  Office Products   \n",
              "1          Dust-Off Compressed Gas Duster, Pack of 4  Office Products   \n",
              "2  Amram Tagger Standard Tag Attaching Tagging Gu...  Office Products   \n",
              "3  AmazonBasics 12-Sheet High-Security Micro-Cut ...  Office Products   \n",
              "4  Derwent Colored Pencils, Inktense Ink Pencils,...  Office Products   \n",
              "\n",
              "   star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
              "0            5              0            0    N                 Y   \n",
              "1            5              0            1    N                 Y   \n",
              "2            5              0            0    N                 Y   \n",
              "3            1              2            3    N                 Y   \n",
              "4            4              0            0    N                 Y   \n",
              "\n",
              "                                     review_headline  \\\n",
              "0                                         Five Stars   \n",
              "1  Phffffffft, Phfffffft. Lots of air, and it's C...   \n",
              "2                      but I am sure I will like it.   \n",
              "3  and the shredder was dirty and the bin was par...   \n",
              "4                                         Four Stars   \n",
              "\n",
              "                                         review_body review_date  \n",
              "0                                     Great product.  2015-08-31  \n",
              "1  What's to say about this commodity item except...  2015-08-31  \n",
              "2    Haven't used yet, but I am sure I will like it.  2015-08-31  \n",
              "3  Although this was labeled as &#34;new&#34; the...  2015-08-31  \n",
              "4                    Gorgeous colors and easy to use  2015-08-31  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_srjcT8xDoK"
      },
      "source": [
        "## Keep Reviews and Ratings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sj1wkrmHxDoK"
      },
      "source": [
        "For now ['star_rating','review_headline','review_body'] has been loaded. 'review_headline' has been added with 'review_body' and the whole text is used for sentiment classification and 'review_headline' is dropped.  \n",
        "Adding the headline with the body increases the accuracy by 2%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdyZPJCVxDoK",
        "outputId": "c6664871-5afe-44fa-af34-ee5764385ba0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Aditya Roy\\AppData\\Local\\Temp\\ipykernel_10004\\3849179413.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset[\"review_body\"] = data[\"review_headline\"] + \": \" + data[\"review_body\"]\n",
            "C:\\Users\\Aditya Roy\\AppData\\Local\\Temp\\ipykernel_10004\\3849179413.py:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset['review_body'] = dataset['review_body'].apply(str)\n"
          ]
        }
      ],
      "source": [
        "dataset = data[['star_rating', 'review_body']]\n",
        "\n",
        "dataset[\"review_body\"] = data[\"review_headline\"] + \": \" + data[\"review_body\"]\n",
        "dataset['review_body'] = dataset['review_body'].apply(str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDD_xhUTxDoL"
      },
      "source": [
        "1 - Positive  \n",
        "0 - Negative  \n",
        "2 - Neutral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAQhJRUoxDoL",
        "outputId": "34dc6385-4480-4ced-8050-1d821d177b31"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Aditya Roy\\AppData\\Local\\Temp\\ipykernel_10004\\4046678660.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dataset['labels'] = dataset['star_rating'].apply(createLabel)\n"
          ]
        }
      ],
      "source": [
        "def createLabel(r):\n",
        "    if r>3:\n",
        "        return 1\n",
        "    if r == 3:   #neutral\n",
        "        return 2\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "dataset['labels'] = dataset['star_rating'].apply(createLabel)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S16KWCN0xDoM"
      },
      "outputs": [],
      "source": [
        "# Drop NaN values\n",
        "dataset = dataset.dropna(subset=['review_body']).reset_index(drop=True)\n",
        "dataset = dataset.dropna(subset=['star_rating']).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opqn13LmxDoN"
      },
      "source": [
        "Random state specified to facilitate easy replication of results. Random state = 42 has been used throughout this notebook for same reason."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNOZX1zNxDoN"
      },
      "outputs": [],
      "source": [
        "\n",
        "R1 = dataset[dataset['star_rating']==1].sample(50000, random_state=42)\n",
        "R2 = dataset[dataset['star_rating']==2].sample(50000, random_state=42)\n",
        "R3 = dataset[dataset['star_rating']==3].sample(50000, random_state=42)\n",
        "R4 = dataset[dataset['star_rating']==4].sample(50000, random_state=42)\n",
        "R5 = dataset[dataset['star_rating']==5].sample(50000, random_state=42)\n",
        "\n",
        "# BinDataset = pd.concat([R1,R2,R4,R5], ignore_index=True)\n",
        "ter_dataset = pd.concat([R1,R2,R3,R4,R5], ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RQ7HHubxDoN"
      },
      "source": [
        "# Data Cleaning\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIVI74YRxDoO"
      },
      "outputs": [],
      "source": [
        "# Apply gensim's text preprocessing\n",
        "ter_dataset[\"tokens\"] = ter_dataset[\"review_body\"].apply(lambda x: simple_preprocess(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4yfmKPIxDoO",
        "outputId": "2d7f6281-4080-4b0d-f0f6-82dcc600bc62"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "del R1,R2,R3,R4,R5, dataset, data\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HsjEc9BxDoP"
      },
      "source": [
        "# Pretrained Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7eMjWFvxDoP"
      },
      "outputs": [],
      "source": [
        "word2vec = api.load(\"word2vec-google-news-300\")\n",
        "\n",
        "def get_average_word2vec(tokens, model, vector_size=300):\n",
        "    vectors = [np.float32(model[word]) for word in tokens if word in model]\n",
        "    return np.float32(np.mean(vectors, axis=0)) if vectors else np.float32(np.zeros(vector_size))\n",
        "\n",
        "def get_10_word2vec(tokens, model, vector_size=300):\n",
        "\n",
        "    vectors = [np.float32(model[word]) for word in tokens if word in model][:10]\n",
        "\n",
        "    while len(vectors) < 10:\n",
        "        vectors.append(np.float32(np.zeros(vector_size)))\n",
        "\n",
        "    return np.concatenate(vectors, axis=0)\n",
        "\n",
        "\n",
        "# FOR CNN\n",
        "def get_average_50_word2vec(tokens, model, vector_size=300):\n",
        "    vectors = [np.float32(model[word]) for word in tokens if word in model][:50]\n",
        "    return np.float32(np.mean(vectors, axis=0)) if vectors else np.float32(np.zeros(vector_size))\n",
        "\n",
        "ter_dataset[\"pretrained_average_embeddings\"] = ter_dataset[\"tokens\"].apply(lambda x: get_average_word2vec(x, word2vec))\n",
        "ter_dataset[\"pretrained_average_50_embeddings\"] = ter_dataset[\"tokens\"].apply(lambda x: get_average_50_word2vec(x, word2vec))\n",
        "ter_dataset[\"pretrained_top_embeddings\"] = ter_dataset[\"tokens\"].apply(lambda x: get_10_word2vec(x, word2vec))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdEfQrGyxDoQ"
      },
      "source": [
        "# Custom Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XK94U_iBxDoQ"
      },
      "outputs": [],
      "source": [
        "custom_word2vec = Word2Vec(sentences=ter_dataset[\"tokens\"], vector_size=300, window=5, min_count=1, workers=4)\n",
        "\n",
        "def get_average_word2vec(tokens, model, vector_size=300):\n",
        "    vectors = [np.float32(model.wv[word]) for word in tokens if word in model.wv]\n",
        "    return np.float32(np.mean(vectors, axis=0)) if vectors else np.float32(np.zeros(vector_size))\n",
        "\n",
        "def get_10_word2vec(tokens, model, vector_size=300):\n",
        "\n",
        "    vectors = [np.float32(model.wv[word]) for word in tokens if word in model.wv][:10]\n",
        "\n",
        "    while len(vectors) < 10:\n",
        "        vectors.append(np.float32(np.zeros(vector_size)))\n",
        "\n",
        "    return np.concatenate(vectors, axis=0)\n",
        "\n",
        "\n",
        "# FOR CNN\n",
        "def get_average_50_word2vec(tokens, model, vector_size=300):\n",
        "    vectors = [np.float32(model[word]) for word in tokens if word in model][:50]\n",
        "    return np.float32(np.mean(vectors, axis=0)) if vectors else np.float32(np.zeros(vector_size))\n",
        "\n",
        "ter_dataset[\"custom_average_embeddings\"] = ter_dataset[\"tokens\"].apply(lambda x: get_average_word2vec(x, custom_word2vec))\n",
        "ter_dataset[\"custom_average_50_embeddings\"] = ter_dataset[\"tokens\"].apply(lambda x: get_average_50_word2vec(x, word2vec))\n",
        "ter_dataset[\"custom_top_embeddings\"] = ter_dataset[\"tokens\"].apply(lambda x: get_10_word2vec(x, custom_word2vec))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEBFeEkyxDoR",
        "outputId": "3cda3ec9-2b4e-422e-ee92-993720661b0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "most similar to Male - Man + Woman = [('female', 0.8426273465156555)]\n",
            "Similarity between Outstanding and Excelent = 0.5567485690116882\n"
          ]
        }
      ],
      "source": [
        "out = word2vec.most_similar(positive=[\"male\",\"woman\"],negative=[\"man\"], topn=1)\n",
        "print(f\"most similar to Male - Man + Woman = {out}\")\n",
        "print(f\"Similarity between Outstanding and Excelent = {word2vec.similarity('outstanding','excellent')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfGRe4qaxDoR",
        "outputId": "d1beb333-b5a6-4cf5-b250-3ee424048726"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "most similar to Male - Man + Woman = [('female', 0.7385611534118652)]\n",
            "Similarity between Outstanding and Excelent = 0.8139455318450928\n"
          ]
        }
      ],
      "source": [
        "out = custom_word2vec.wv.most_similar(positive=[\"male\",\"woman\"],negative=[\"man\"],topn=1)\n",
        "print(f\"most similar to Male - Man + Woman = {out}\")\n",
        "print(f\"Similarity between Outstanding and Excelent = {custom_word2vec.wv.similarity('outstanding','excellent')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqE6iJF7xDoR"
      },
      "source": [
        "The pretrained embeddings demonstrate stronger performance in capturing fundamental patterns, reflecting their training on general-purpose text. Meanwhile, the custom embeddings show superior sensitivity to domain-specific words, evidenced by the significantly higher similarity between Outstanding and Excellent. This suggests the custom model better reflects contextual relationships in its training data, while the pretrained model preserves more robust grammatical/semantic regularities. The contrast highlights the trade-off between general linguistic knowledge (pretrained) and task/domain-specific optimization (custom)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBnsG2iIxDoS"
      },
      "source": [
        "#### Creating Binary Dataset by dropping the neutral label (2 or star_rating == 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBUWvnQ6xDoS"
      },
      "outputs": [],
      "source": [
        "bin_dataset = ter_dataset[ter_dataset['star_rating']!=3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4R9-dnVXxDoS"
      },
      "source": [
        "# Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jpnlx-HxDoT"
      },
      "source": [
        "## remove the stop words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxIJxZp5xDoT"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "# nltk.download('punkt_tab')\n",
        "# nltk.download('stopwords')\n",
        "\n",
        "def removeStopwords(review):\n",
        "    tokens = nltk.tokenize.word_tokenize(review)\n",
        "    filtered_tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "\n",
        "\n",
        "dataset['review_body'] = dataset['review_body'].apply(removeStopwords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxwltLNCxDoT"
      },
      "source": [
        "## perform lemmatization  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUbUbPPWxDoT"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_text(review):\n",
        "    tokens = nltk.tokenize.word_tokenize(review)\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "dataset['review_body'] = dataset['review_body'].apply(lemmatize_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvvioARSxDoT"
      },
      "outputs": [],
      "source": [
        "#SPlitting\n",
        "\n",
        "X_trainB, X_testB, y_trainB, y_testB = train_test_split(bin_dataset[['pretrained_top_embeddings', 'pretrained_average_embeddings', 'custom_average_embeddings', 'custom_top_embeddings','pretrained_average_50_embeddings','custom_average_50_embeddings']], bin_dataset['labels'], test_size=0.2, random_state=42)\n",
        "X_trainT, X_testT, y_trainT, y_testT = train_test_split(ter_dataset[['pretrained_top_embeddings', 'pretrained_average_embeddings', 'custom_average_embeddings', 'custom_top_embeddings','pretrained_average_50_embeddings','custom_average_50_embeddings']], ter_dataset['labels'], test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cErUg1koxDoU",
        "outputId": "374900a3-7b9c-469c-fa7c-9cf249351db3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "516"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "del bin_dataset, ter_dataset\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31JZM74YxDoU"
      },
      "source": [
        "Following function was created to print the metrics as per the specified format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nq_iXHMNxDoU"
      },
      "outputs": [],
      "source": [
        "def evaluate(y_true, y_pred, dataset_name = None):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"{dataset_name} Metrics:\")\n",
        "    print(\"accuracy, precision, recall, f1\")\n",
        "    print(\"{:.4f}\".format(accuracy), \"{:.4f}\".format(precision), \"{:.4f}\".format(recall), \"{:.4f}\".format(f1))\n",
        "\n",
        "\n",
        "#### Multi class classification\n",
        "def evaluate_multi(y_true, y_pred, dataset_name = None):\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, average='macro')\n",
        "    recall = recall_score(y_true, y_pred, average='macro')\n",
        "    f1 = f1_score(y_true, y_pred, average='macro')\n",
        "\n",
        "    print(f\"{dataset_name} Metrics:\")\n",
        "    print(\"accuracy, precision, recall, f1\")\n",
        "    print(\"{:.4f}\".format(accuracy), \"{:.4f}\".format(precision), \"{:.4f}\".format(recall), \"{:.4f}\".format(f1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGN7HnUoxDoV"
      },
      "source": [
        "# Simple Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKjpQWMZxDoV"
      },
      "source": [
        "## Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhTQvPqBxDoV"
      },
      "source": [
        "Perceptron was trained and tested for different max_iter values. It was found that in every case the model gives optimal output with just 100 iterations. Hence, 100 was chosen as the final number of iterations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_sZ0g-3xDoV"
      },
      "source": [
        "### Word2Vec features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfrHq_88xDoV"
      },
      "source": [
        "#### Custom Word2Vec model features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tJ_qgLbxDoW",
        "outputId": "f3628ebc-eda6-4346-db65-0579b6dd334d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Perceptron(max_iter=100, random_state=42)"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "perceptron_custom = Perceptron(max_iter=100, tol=1e-3, random_state=42)\n",
        "perceptron_custom.fit(np.stack(X_trainB['custom_average_embeddings'].values), y_trainB.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZOzNHxPxDoX"
      },
      "outputs": [],
      "source": [
        "y_test_pred = perceptron_custom.predict(np.stack(X_testB['custom_average_embeddings'].values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqsH9LOZxDoX",
        "outputId": "6ada79e3-9e5c-4fc9-aa07-cae6e1c50fa6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perceptron on Custom embeddings Metrics:\n",
            "accuracy, precision, recall, f1\n",
            "0.8506 0.9363 0.7526 0.8344\n"
          ]
        }
      ],
      "source": [
        "evaluate(y_testB.values, y_test_pred, \"Perceptron on Custom embeddings\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRnVV_tcxDoY"
      },
      "source": [
        "#### Pretrained Word2Vec model features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_BhSM7oxDoY",
        "outputId": "a4d9abfd-9fb3-4add-b39d-af5933c115c0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Perceptron(max_iter=100, random_state=42)"
            ]
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "perceptron_pretrained = Perceptron(max_iter=100, tol=1e-3, random_state=42)\n",
        "perceptron_pretrained.fit(np.stack(X_trainB['pretrained_average_embeddings'].values), y_trainB.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAklLYJHxDoY"
      },
      "outputs": [],
      "source": [
        "y_test_pred = perceptron_pretrained.predict(np.stack(X_testB['pretrained_average_embeddings'].values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeZ2-Ej0xDoY",
        "outputId": "8aedfe47-19fc-44c1-f725-c261bcbe08a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perceptron on pretrained embeddings Metrics:\n",
            "accuracy, precision, recall, f1\n",
            "0.8458 0.8071 0.9089 0.8550\n"
          ]
        }
      ],
      "source": [
        "evaluate(y_testB.values, y_test_pred, \"Perceptron on pretrained embeddings\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLJf9RbxxDoZ"
      },
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iu1up4UrxDoZ"
      },
      "source": [
        "## Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5iFhCnCxDoZ"
      },
      "source": [
        "### Custom Word2Vec Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDBESTLTxDoZ",
        "outputId": "edb4dc94-367e-42ab-fc4c-dba83c4633a9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Aditya Roy\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "LinearSVC(random_state=42)"
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "SVM_custom = LinearSVC(random_state=42)\n",
        "SVM_custom.fit(np.stack(X_trainB['custom_average_embeddings'].values), y_trainB.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLfXSbKIxDoa"
      },
      "outputs": [],
      "source": [
        "y_test_pred = SVM_custom.predict(np.stack(X_testB['custom_average_embeddings'].values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f43cZt3axDoa",
        "outputId": "a288ec9d-f474-4b27-9cc2-4d48c5a5576e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(160000,)"
            ]
          },
          "execution_count": 111,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_trainB.values.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nO1Bv51bxDoa",
        "outputId": "b4c65be8-75ee-4abd-912c-0315cd504b18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM on Custom embeddings: Metrics:\n",
            "accuracy, precision, recall, f1\n",
            "0.8985 0.9104 0.8840 0.8970\n"
          ]
        }
      ],
      "source": [
        "evaluate(y_testB.values, y_test_pred, \"SVM on Custom embeddings:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtROVEs7xDoa"
      },
      "source": [
        "### Pretrained Word2Vec Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Bb3OsxxxDob",
        "outputId": "f1d14fa0-3daf-43d4-b262-53d3e4a37b6e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LinearSVC(random_state=42)"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "SVM_pretrained = LinearSVC(random_state=42)\n",
        "SVM_pretrained.fit(np.stack(X_trainB['pretrained_average_embeddings'].values), y_trainB.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdTwk6mbxDob"
      },
      "outputs": [],
      "source": [
        "y_test_pred = SVM_pretrained.predict(np.stack(X_testB['pretrained_average_embeddings'].values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1rYf4FLxDob",
        "outputId": "b98f49c5-dbe4-4905-b3e7-18d6934f4f1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM on pretrained embeddings Metrics:\n",
            "accuracy, precision, recall, f1\n",
            "0.8657 0.8836 0.8425 0.8625\n"
          ]
        }
      ],
      "source": [
        "evaluate(y_testB.values, y_test_pred, \"SVM on pretrained embeddings\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q40p5TDGxDoc"
      },
      "source": [
        "## Simple model comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "573ES1w_xDoc",
        "outputId": "aa187dd6-d650-4e2e-e712-ddcc37ed7770"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_name</th>\n",
              "      <th>Pretrained_embeddings</th>\n",
              "      <th>Custom_embeddings</th>\n",
              "      <th>TF-IDF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Perceptron</td>\n",
              "      <td>0.8458</td>\n",
              "      <td>0.8548</td>\n",
              "      <td>0.8871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SVM</td>\n",
              "      <td>0.8657</td>\n",
              "      <td>0.8985</td>\n",
              "      <td>0.9269</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   model_name  Pretrained_embeddings  Custom_embeddings  TF-IDF\n",
              "0  Perceptron                 0.8458             0.8548  0.8871\n",
              "1         SVM                 0.8657             0.8985  0.9269"
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "acc = {\n",
        "    \"model_name\":[\"Perceptron\",\"SVM\"],\n",
        "    \"Pretrained_embeddings\":[0.8458,0.8657],\n",
        "    \"Custom_embeddings\":[0.8548,0.8985],\n",
        "    \"TF-IDF\":[0.8871,0.9269]\n",
        "}\n",
        "\n",
        "acc = pd.DataFrame(acc)\n",
        "acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdZFtRUIxDoc"
      },
      "source": [
        "The scores reveal three key trends:\n",
        "\n",
        "TF-IDF Dominance: Both classifiers achieve their highest performance with TF-IDF features, suggesting traditional bag-of-words approaches remain competitive for this specific task, due to strong keyword signals in the data.\n",
        "\n",
        "Custom Embedding Advantage: Custom embeddings outperform pretrained ones for both models, indicating domain-specific training improves feature relevance despite lower generalization.\n",
        "\n",
        "Model Capacity: The SVM consistently outperforms the Perceptron across all feature types, highlighting its superior ability to handle complex decision boundaries with all input representations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0X1LY3QxDoc"
      },
      "source": [
        "# Feedforward Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYFFcuxsxDoc"
      },
      "outputs": [],
      "source": [
        "class EmbeddingDataset(Dataset):\n",
        "    def __init__(self, X_df, y_df):\n",
        "\n",
        "        self.embeddings = torch.tensor(np.stack(X_df.values), dtype=torch.float32)\n",
        "\n",
        "        self.labels = torch.tensor(y_df.values, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.embeddings[idx], self.labels[idx]\n",
        "\n",
        "# Binary Classification MLP\n",
        "class BinaryMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden1=50, hidden2=10, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden1, hidden2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden2, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x).squeeze()\n",
        "\n",
        "# Ternary Classification MLP\n",
        "class TernaryMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden1=50, hidden2=10, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden1, hidden2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden2, 3)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AprQU7A0xDod"
      },
      "outputs": [],
      "source": [
        "# for internal evaluations\n",
        "def evaluate_(model, data_loader, num_classes, device='cpu'):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    criterion = nn.BCEWithLogitsLoss() if num_classes == 2 else nn.CrossEntropyLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            if num_classes == 2:\n",
        "                loss = criterion(outputs, labels.float())\n",
        "                preds = (torch.sigmoid(outputs) > 0.5).long()\n",
        "            else:\n",
        "                loss = criterion(outputs, labels)\n",
        "                preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / total\n",
        "    accuracy = correct / total\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtBIu9kLxDod"
      },
      "outputs": [],
      "source": [
        "def create_dataloaders(X_df, y_df, test_size=0.2, val_size=0.2, batch_size=64):\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "        X_df, y_df,\n",
        "        test_size=test_size + val_size,\n",
        "        stratify=y_df\n",
        "    )\n",
        "\n",
        "\n",
        "    test_val_size = val_size / (test_size + val_size)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(\n",
        "        X_temp, y_temp,\n",
        "        test_size=test_val_size,\n",
        "        stratify=y_temp\n",
        "    )\n",
        "\n",
        "\n",
        "    train_ds = EmbeddingDataset(X_train, y_train)\n",
        "    val_ds = EmbeddingDataset(X_val, y_val)\n",
        "    test_ds = EmbeddingDataset(X_test, y_test)\n",
        "\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=batch_size)\n",
        "    test_loader = DataLoader(test_ds, batch_size=batch_size)\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81nMgfAZxDod"
      },
      "outputs": [],
      "source": [
        "def get_predictions(model, dataloader, device='cpu'):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            if isinstance(model, BinaryMLP):  # Binary classification\n",
        "                preds = (torch.sigmoid(outputs) > 0.5).cpu().numpy()\n",
        "            else:  # Ternary classification\n",
        "                preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    return np.array(all_labels), np.array(all_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16bcoLSvxDod"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, num_classes=2, num_epochs=20, learning_rate=0.001):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    if num_classes == 2:\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "    else:\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 3\n",
        "    trigger_times = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            if num_classes == 2:\n",
        "                loss = criterion(outputs, labels.float())\n",
        "            else:\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "\n",
        "        # Validation\n",
        "        val_loss, val_acc = evaluate_(model, val_loader, num_classes, device)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print(f'Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}')\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            trigger_times = 0\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "        else:\n",
        "            trigger_times += 1\n",
        "            if trigger_times >= patience:\n",
        "                print(\"Early stopping!\")\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(torch.load('best_model.pth'))\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbRQQFYLxDoe"
      },
      "source": [
        "## Pretrained Word2Vec model features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkI_NWs4xDoe"
      },
      "source": [
        "## Average of Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMHCecXsxDoe"
      },
      "source": [
        "### Binary model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRNyFhFHxDoe",
        "outputId": "2d8383d4-6f45-49d0-a9f4-0f54d94de756"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "Train Loss: 0.4374 | Val Loss: 0.3430 | Val Acc: 0.8552\n",
            "Epoch 2/20\n",
            "Train Loss: 0.3730 | Val Loss: 0.3245 | Val Acc: 0.8576\n",
            "Epoch 3/20\n",
            "Train Loss: 0.3630 | Val Loss: 0.3133 | Val Acc: 0.8655\n",
            "Epoch 4/20\n",
            "Train Loss: 0.3559 | Val Loss: 0.3061 | Val Acc: 0.8702\n",
            "Epoch 5/20\n",
            "Train Loss: 0.3522 | Val Loss: 0.3010 | Val Acc: 0.8732\n",
            "Epoch 6/20\n",
            "Train Loss: 0.3460 | Val Loss: 0.2996 | Val Acc: 0.8740\n",
            "Epoch 7/20\n",
            "Train Loss: 0.3423 | Val Loss: 0.2976 | Val Acc: 0.8744\n",
            "Epoch 8/20\n",
            "Train Loss: 0.3433 | Val Loss: 0.3007 | Val Acc: 0.8728\n",
            "Epoch 9/20\n",
            "Train Loss: 0.3408 | Val Loss: 0.3075 | Val Acc: 0.8640\n",
            "Epoch 10/20\n",
            "Train Loss: 0.3379 | Val Loss: 0.2910 | Val Acc: 0.8788\n",
            "Epoch 11/20\n",
            "Train Loss: 0.3376 | Val Loss: 0.2935 | Val Acc: 0.8748\n",
            "Epoch 12/20\n",
            "Train Loss: 0.3371 | Val Loss: 0.2885 | Val Acc: 0.8778\n",
            "Epoch 13/20\n",
            "Train Loss: 0.3347 | Val Loss: 0.2865 | Val Acc: 0.8788\n",
            "Epoch 14/20\n",
            "Train Loss: 0.3321 | Val Loss: 0.2852 | Val Acc: 0.8808\n",
            "Epoch 15/20\n",
            "Train Loss: 0.3316 | Val Loss: 0.2863 | Val Acc: 0.8789\n",
            "Epoch 16/20\n",
            "Train Loss: 0.3328 | Val Loss: 0.2820 | Val Acc: 0.8836\n",
            "Epoch 17/20\n",
            "Train Loss: 0.3298 | Val Loss: 0.2828 | Val Acc: 0.8799\n",
            "Epoch 18/20\n",
            "Train Loss: 0.3302 | Val Loss: 0.2840 | Val Acc: 0.8819\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Aditya Roy\\AppData\\Local\\Temp\\ipykernel_20652\\3495807950.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('best_model.pth'))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19/20\n",
            "Train Loss: 0.3282 | Val Loss: 0.2835 | Val Acc: 0.8809\n",
            "Early stopping!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "binary_train_loader, binary_val_loader, binary_test_loader = create_dataloaders(X_trainB['pretrained_average_embeddings'], y_trainB, test_size=0.2, val_size=0.2, batch_size=64)\n",
        "binary_model = BinaryMLP(input_dim=len(X_trainB['pretrained_average_embeddings'].iloc[0]))\n",
        "trained_binary_model = train_model(binary_model, binary_train_loader, binary_val_loader, num_classes=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDnuMaDqxDoe",
        "outputId": "35079143-acdc-433b-c6b7-d6cfbaa71dd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pretrained average features on Binary MLP: Metrics:\n",
            "accuracy, precision, recall, f1\n",
            "0.8792 0.8896 0.8660 0.8776\n"
          ]
        }
      ],
      "source": [
        "test_true, test_pred = get_predictions(trained_binary_model, binary_test_loader)\n",
        "evaluate(test_true, test_pred, \"Pretrained average features on Binary MLP:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5D-N_GBfxDof"
      },
      "source": [
        "### Ternary model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmPWB90zxDof",
        "outputId": "950e67d5-9648-4474-dc79-6831b0c768a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "Train Loss: 0.8451 | Val Loss: 0.7292 | Val Acc: 0.6853\n",
            "Epoch 2/20\n",
            "Train Loss: 0.7741 | Val Loss: 0.7215 | Val Acc: 0.6912\n",
            "Epoch 3/20\n",
            "Train Loss: 0.7639 | Val Loss: 0.7133 | Val Acc: 0.6952\n",
            "Epoch 4/20\n",
            "Train Loss: 0.7591 | Val Loss: 0.6982 | Val Acc: 0.6969\n",
            "Epoch 5/20\n",
            "Train Loss: 0.7521 | Val Loss: 0.6988 | Val Acc: 0.6985\n",
            "Epoch 6/20\n",
            "Train Loss: 0.7477 | Val Loss: 0.7023 | Val Acc: 0.6935\n",
            "Epoch 7/20\n",
            "Train Loss: 0.7453 | Val Loss: 0.6918 | Val Acc: 0.7002\n",
            "Epoch 8/20\n",
            "Train Loss: 0.7426 | Val Loss: 0.6871 | Val Acc: 0.7001\n",
            "Epoch 9/20\n",
            "Train Loss: 0.7411 | Val Loss: 0.6839 | Val Acc: 0.7016\n",
            "Epoch 10/20\n",
            "Train Loss: 0.7391 | Val Loss: 0.6804 | Val Acc: 0.7025\n",
            "Epoch 11/20\n",
            "Train Loss: 0.7355 | Val Loss: 0.6825 | Val Acc: 0.7036\n",
            "Epoch 12/20\n",
            "Train Loss: 0.7376 | Val Loss: 0.6819 | Val Acc: 0.6993\n",
            "Epoch 13/20\n",
            "Train Loss: 0.7355 | Val Loss: 0.6832 | Val Acc: 0.7048\n",
            "Early stopping!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Aditya Roy\\AppData\\Local\\Temp\\ipykernel_20652\\3495807950.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('best_model.pth'))\n"
          ]
        }
      ],
      "source": [
        "\n",
        "ternary_train_loader, ternary_val_loader, ternary_test_loader = create_dataloaders(X_trainT['pretrained_average_embeddings'], y_trainT, test_size=0.2, val_size=0.2, batch_size=64)\n",
        "ternary_model = TernaryMLP(input_dim=len(X_trainT['pretrained_average_embeddings'].iloc[0]))\n",
        "trained_ternary_model = train_model(ternary_model, ternary_train_loader, ternary_val_loader, num_classes=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8h-593PHxDof",
        "outputId": "8df1166b-250e-4a6c-850b-c50f48683be4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pretrained average features on ternary MLP: Metrics:\n",
            "accuracy, precision, recall, f1\n",
            "0.7022 0.4685 0.5850 0.5202\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Aditya Roy\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "test_true, test_pred = get_predictions(trained_ternary_model, ternary_test_loader)\n",
        "evaluate_multi(test_true, test_pred, \"Pretrained average features on ternary MLP:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKGKMXUgxDof"
      },
      "source": [
        "## First 10 Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR3M3oWGxDof"
      },
      "source": [
        "### Binary Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enN5e-GtxDog",
        "outputId": "49c0731f-d852-4f2e-a8d4-6c67f85989ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "Train Loss: 0.4072 | Val Loss: 0.3030 | Val Acc: 0.8666\n",
            "Epoch 2/20\n",
            "Train Loss: 0.3462 | Val Loss: 0.2956 | Val Acc: 0.8696\n",
            "Epoch 3/20\n",
            "Train Loss: 0.3300 | Val Loss: 0.2875 | Val Acc: 0.8742\n",
            "Epoch 4/20\n",
            "Train Loss: 0.3166 | Val Loss: 0.2837 | Val Acc: 0.8753\n",
            "Epoch 5/20\n",
            "Train Loss: 0.3072 | Val Loss: 0.2810 | Val Acc: 0.8788\n",
            "Epoch 6/20\n",
            "Train Loss: 0.2975 | Val Loss: 0.2813 | Val Acc: 0.8785\n",
            "Epoch 7/20\n",
            "Train Loss: 0.2893 | Val Loss: 0.2801 | Val Acc: 0.8786\n",
            "Epoch 8/20\n",
            "Train Loss: 0.2806 | Val Loss: 0.2801 | Val Acc: 0.8789\n",
            "Epoch 9/20\n",
            "Train Loss: 0.2791 | Val Loss: 0.2825 | Val Acc: 0.8803\n",
            "Epoch 10/20\n",
            "Train Loss: 0.2735 | Val Loss: 0.2876 | Val Acc: 0.8783\n",
            "Early stopping!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Aditya Roy\\AppData\\Local\\Temp\\ipykernel_20652\\3495807950.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('best_model.pth'))\n"
          ]
        }
      ],
      "source": [
        "\n",
        "binary_train_loader, binary_val_loader, binary_test_loader = create_dataloaders(X_trainB['pretrained_top_embeddings'], y_trainB, test_size=0.2, val_size=0.2, batch_size=64)\n",
        "binary_model = BinaryMLP(input_dim=len(X_trainB['pretrained_top_embeddings'].iloc[0]))\n",
        "trained_binary_model = train_model(binary_model, binary_train_loader, binary_val_loader, num_classes=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKiB67L_xDog",
        "outputId": "090b4e91-5403-4ecc-e1b2-d30b4a3d66b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pretrained top 10 features on Binary MLP: Metrics:\n",
            "accuracy, precision, recall, f1\n",
            "0.8809 0.8667 0.9002 0.8831\n"
          ]
        }
      ],
      "source": [
        "test_true, test_pred = get_predictions(trained_binary_model, binary_test_loader)\n",
        "evaluate(test_true, test_pred, \"Pretrained top 10 features on Binary MLP:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDYkyPutxDog"
      },
      "source": [
        "### Ternary Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ng0ESoRxDog",
        "outputId": "ec04379f-3239-419f-a4a9-1dcf598d93ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "Train Loss: 0.7968 | Val Loss: 0.6920 | Val Acc: 0.6956\n",
            "Epoch 2/20\n",
            "Train Loss: 0.7332 | Val Loss: 0.6587 | Val Acc: 0.7137\n",
            "Epoch 3/20\n",
            "Train Loss: 0.7153 | Val Loss: 0.6403 | Val Acc: 0.7250\n",
            "Epoch 4/20\n",
            "Train Loss: 0.6999 | Val Loss: 0.6339 | Val Acc: 0.7292\n",
            "Epoch 5/20\n",
            "Train Loss: 0.6912 | Val Loss: 0.6303 | Val Acc: 0.7321\n",
            "Epoch 6/20\n",
            "Train Loss: 0.6858 | Val Loss: 0.6361 | Val Acc: 0.7328\n",
            "Epoch 7/20\n",
            "Train Loss: 0.6778 | Val Loss: 0.6294 | Val Acc: 0.7335\n",
            "Epoch 8/20\n",
            "Train Loss: 0.6714 | Val Loss: 0.6244 | Val Acc: 0.7348\n",
            "Epoch 9/20\n",
            "Train Loss: 0.6726 | Val Loss: 0.6193 | Val Acc: 0.7340\n",
            "Epoch 10/20\n",
            "Train Loss: 0.6644 | Val Loss: 0.6190 | Val Acc: 0.7374\n",
            "Epoch 11/20\n",
            "Train Loss: 0.6619 | Val Loss: 0.6217 | Val Acc: 0.7357\n",
            "Epoch 12/20\n",
            "Train Loss: 0.6582 | Val Loss: 0.6181 | Val Acc: 0.7380\n",
            "Epoch 13/20\n",
            "Train Loss: 0.6586 | Val Loss: 0.6207 | Val Acc: 0.7346\n",
            "Epoch 14/20\n",
            "Train Loss: 0.6545 | Val Loss: 0.6224 | Val Acc: 0.7361\n",
            "Epoch 15/20\n",
            "Train Loss: 0.6507 | Val Loss: 0.6168 | Val Acc: 0.7385\n",
            "Epoch 16/20\n",
            "Train Loss: 0.6491 | Val Loss: 0.6225 | Val Acc: 0.7386\n",
            "Epoch 17/20\n",
            "Train Loss: 0.6474 | Val Loss: 0.6173 | Val Acc: 0.7389\n",
            "Epoch 18/20\n",
            "Train Loss: 0.6458 | Val Loss: 0.6196 | Val Acc: 0.7391\n",
            "Early stopping!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Aditya Roy\\AppData\\Local\\Temp\\ipykernel_20652\\3495807950.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('best_model.pth'))\n"
          ]
        }
      ],
      "source": [
        "ternary_train_loader, ternary_val_loader, ternary_test_loader = create_dataloaders(X_trainT['pretrained_top_embeddings'], y_trainT, test_size=0.2, val_size=0.2, batch_size=64)\n",
        "ternary_model = TernaryMLP(input_dim=len(X_trainT['pretrained_top_embeddings'].iloc[0]))\n",
        "trained_ternary_model = train_model(ternary_model, ternary_train_loader, ternary_val_loader, num_classes=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHsmZGm3xDoh",
        "outputId": "74555408-ccc9-42d3-9c28-3c2e8cb277d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pretrained top 10 features on ternary MLP: Metrics:\n",
            "accuracy, precision, recall, f1\n",
            "0.7413 0.7844 0.6513 0.6424\n"
          ]
        }
      ],
      "source": [
        "test_true, test_pred = get_predictions(trained_ternary_model, ternary_test_loader)\n",
        "evaluate_multi(test_true, test_pred, \"Pretrained top 10 features on ternary MLP:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAcAUhRNxDoh"
      },
      "source": [
        "## Custom Word2Vec model features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjO6quPnxDoh"
      },
      "source": [
        "## Average of Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qnCzTbJxDoh"
      },
      "source": [
        "#### Binary model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjWwbVWrxDoh",
        "outputId": "0ceee5a8-e76b-4d45-937a-5fd03d96d2c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "Train Loss: 0.3617 | Val Loss: 0.2579 | Val Acc: 0.8932\n",
            "Epoch 2/20\n",
            "Train Loss: 0.3074 | Val Loss: 0.2470 | Val Acc: 0.8969\n",
            "Epoch 3/20\n",
            "Train Loss: 0.2976 | Val Loss: 0.2448 | Val Acc: 0.9010\n",
            "Epoch 4/20\n",
            "Train Loss: 0.2925 | Val Loss: 0.2402 | Val Acc: 0.9035\n",
            "Epoch 5/20\n",
            "Train Loss: 0.2890 | Val Loss: 0.2377 | Val Acc: 0.9046\n",
            "Epoch 6/20\n",
            "Train Loss: 0.2862 | Val Loss: 0.2357 | Val Acc: 0.9032\n",
            "Epoch 7/20\n",
            "Train Loss: 0.2815 | Val Loss: 0.2334 | Val Acc: 0.9059\n",
            "Epoch 8/20\n",
            "Train Loss: 0.2814 | Val Loss: 0.2378 | Val Acc: 0.8995\n",
            "Epoch 9/20\n",
            "Train Loss: 0.2815 | Val Loss: 0.2344 | Val Acc: 0.9059\n",
            "Epoch 10/20\n",
            "Train Loss: 0.2784 | Val Loss: 0.2305 | Val Acc: 0.9051\n",
            "Epoch 11/20\n",
            "Train Loss: 0.2793 | Val Loss: 0.2317 | Val Acc: 0.9041\n",
            "Epoch 12/20\n",
            "Train Loss: 0.2775 | Val Loss: 0.2316 | Val Acc: 0.9069\n",
            "Epoch 13/20\n",
            "Train Loss: 0.2770 | Val Loss: 0.2276 | Val Acc: 0.9088\n",
            "Epoch 14/20\n",
            "Train Loss: 0.2755 | Val Loss: 0.2304 | Val Acc: 0.9073\n",
            "Epoch 15/20\n",
            "Train Loss: 0.2748 | Val Loss: 0.2276 | Val Acc: 0.9067\n",
            "Epoch 16/20\n",
            "Train Loss: 0.2744 | Val Loss: 0.2268 | Val Acc: 0.9100\n",
            "Epoch 17/20\n",
            "Train Loss: 0.2732 | Val Loss: 0.2299 | Val Acc: 0.9092\n",
            "Epoch 18/20\n",
            "Train Loss: 0.2729 | Val Loss: 0.2283 | Val Acc: 0.9060\n",
            "Epoch 19/20\n",
            "Train Loss: 0.2714 | Val Loss: 0.2273 | Val Acc: 0.9075\n",
            "Early stopping!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Aditya Roy\\AppData\\Local\\Temp\\ipykernel_20652\\3495807950.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('best_model.pth'))\n"
          ]
        }
      ],
      "source": [
        "\n",
        "binary_train_loader, binary_val_loader, binary_test_loader = create_dataloaders(X_trainB['custom_average_embeddings'], y_trainB, test_size=0.2, val_size=0.2, batch_size=64)\n",
        "binary_model = BinaryMLP(input_dim=len(X_trainB['custom_average_embeddings'].iloc[0]))\n",
        "trained_binary_model = train_model(binary_model, binary_train_loader, binary_val_loader, num_classes=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RO-kk7dpxDoh",
        "outputId": "c828b004-4172-4fdb-898f-1ee16b4bf3e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Custom average features on Binary MLP: Metrics:\n",
            "accuracy, precision, recall, f1\n",
            "0.9061 0.9061 0.9061 0.9061\n"
          ]
        }
      ],
      "source": [
        "test_true, test_pred = get_predictions(trained_binary_model, binary_test_loader)\n",
        "evaluate(test_true, test_pred, \"Custom average features on Binary MLP:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytxcdwJ9xDoi"
      },
      "source": [
        "#### Ternary model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L94IAXJexDoi",
        "outputId": "4cb31b27-ba30-4cfa-f50d-85ff55601200"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "Train Loss: 0.7493 | Val Loss: 0.6300 | Val Acc: 0.7320\n",
            "Epoch 2/20\n",
            "Train Loss: 0.6935 | Val Loss: 0.6022 | Val Acc: 0.7483\n",
            "Epoch 3/20\n",
            "Train Loss: 0.6802 | Val Loss: 0.5990 | Val Acc: 0.7511\n",
            "Epoch 4/20\n",
            "Train Loss: 0.6713 | Val Loss: 0.5988 | Val Acc: 0.7513\n",
            "Epoch 5/20\n",
            "Train Loss: 0.6677 | Val Loss: 0.5893 | Val Acc: 0.7509\n",
            "Epoch 6/20\n",
            "Train Loss: 0.6632 | Val Loss: 0.5862 | Val Acc: 0.7525\n",
            "Epoch 7/20\n",
            "Train Loss: 0.6607 | Val Loss: 0.5803 | Val Acc: 0.7542\n",
            "Epoch 8/20\n",
            "Train Loss: 0.6598 | Val Loss: 0.5766 | Val Acc: 0.7615\n",
            "Epoch 9/20\n",
            "Train Loss: 0.6557 | Val Loss: 0.5810 | Val Acc: 0.7567\n",
            "Epoch 10/20\n",
            "Train Loss: 0.6541 | Val Loss: 0.5761 | Val Acc: 0.7572\n",
            "Epoch 11/20\n",
            "Train Loss: 0.6513 | Val Loss: 0.5748 | Val Acc: 0.7576\n",
            "Epoch 12/20\n",
            "Train Loss: 0.6496 | Val Loss: 0.5779 | Val Acc: 0.7585\n",
            "Epoch 13/20\n",
            "Train Loss: 0.6509 | Val Loss: 0.5706 | Val Acc: 0.7630\n",
            "Epoch 14/20\n",
            "Train Loss: 0.6506 | Val Loss: 0.5728 | Val Acc: 0.7565\n",
            "Epoch 15/20\n",
            "Train Loss: 0.6491 | Val Loss: 0.5727 | Val Acc: 0.7584\n",
            "Epoch 16/20\n",
            "Train Loss: 0.6470 | Val Loss: 0.5704 | Val Acc: 0.7597\n",
            "Epoch 17/20\n",
            "Train Loss: 0.6485 | Val Loss: 0.5724 | Val Acc: 0.7658\n",
            "Epoch 18/20\n",
            "Train Loss: 0.6465 | Val Loss: 0.5686 | Val Acc: 0.7628\n",
            "Epoch 19/20\n",
            "Train Loss: 0.6452 | Val Loss: 0.5709 | Val Acc: 0.7615\n",
            "Epoch 20/20\n",
            "Train Loss: 0.6454 | Val Loss: 0.5710 | Val Acc: 0.7618\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Aditya Roy\\AppData\\Local\\Temp\\ipykernel_20652\\3495807950.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('best_model.pth'))\n"
          ]
        }
      ],
      "source": [
        "ternary_train_loader, ternary_val_loader, ternary_test_loader = create_dataloaders(X_trainT['custom_average_embeddings'], y_trainT, test_size=0.2, val_size=0.2, batch_size=64)\n",
        "ternary_model = TernaryMLP(input_dim=len(X_trainT['custom_average_embeddings'].iloc[0]))\n",
        "trained_ternary_model = train_model(ternary_model, ternary_train_loader, ternary_val_loader, num_classes=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJ5DJVxUxDoi",
        "outputId": "9f2de755-f08c-4534-e116-c51ac2f0dc33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Custom Average features on ternary MLP: Metrics:\n",
            "accuracy, precision, recall, f1\n",
            "0.7620 0.7661 0.6718 0.6639\n"
          ]
        }
      ],
      "source": [
        "test_true, test_pred = get_predictions(trained_ternary_model, ternary_test_loader)\n",
        "evaluate_multi(test_true, test_pred, \"Custom Average features on ternary MLP:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhIQbjDcxDoi"
      },
      "source": [
        "## First 10 Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX7zIAzwxDoi"
      },
      "source": [
        "#### Binary model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWGvtfgLxDoj",
        "outputId": "19e918cf-9d96-4903-ab8f-1a6d5fb21223"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "Train Loss: 0.3750 | Val Loss: 0.2823 | Val Acc: 0.8747\n",
            "Epoch 2/20\n",
            "Train Loss: 0.3232 | Val Loss: 0.2750 | Val Acc: 0.8802\n",
            "Epoch 3/20\n",
            "Train Loss: 0.3113 | Val Loss: 0.2689 | Val Acc: 0.8809\n",
            "Epoch 4/20\n",
            "Train Loss: 0.3012 | Val Loss: 0.2673 | Val Acc: 0.8829\n",
            "Epoch 5/20\n",
            "Train Loss: 0.2946 | Val Loss: 0.2669 | Val Acc: 0.8834\n",
            "Epoch 6/20\n",
            "Train Loss: 0.2884 | Val Loss: 0.2641 | Val Acc: 0.8878\n",
            "Epoch 7/20\n",
            "Train Loss: 0.2831 | Val Loss: 0.2640 | Val Acc: 0.8866\n",
            "Epoch 8/20\n",
            "Train Loss: 0.2781 | Val Loss: 0.2638 | Val Acc: 0.8868\n",
            "Epoch 9/20\n",
            "Train Loss: 0.2708 | Val Loss: 0.2621 | Val Acc: 0.8884\n",
            "Epoch 10/20\n",
            "Train Loss: 0.2675 | Val Loss: 0.2605 | Val Acc: 0.8883\n",
            "Epoch 11/20\n",
            "Train Loss: 0.2664 | Val Loss: 0.2604 | Val Acc: 0.8886\n",
            "Epoch 12/20\n",
            "Train Loss: 0.2626 | Val Loss: 0.2652 | Val Acc: 0.8858\n",
            "Epoch 13/20\n",
            "Train Loss: 0.2596 | Val Loss: 0.2660 | Val Acc: 0.8870\n",
            "Epoch 14/20\n",
            "Train Loss: 0.2585 | Val Loss: 0.2652 | Val Acc: 0.8884\n",
            "Early stopping!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Aditya Roy\\AppData\\Local\\Temp\\ipykernel_20652\\3495807950.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('best_model.pth'))\n"
          ]
        }
      ],
      "source": [
        "\n",
        "binary_train_loader, binary_val_loader, binary_test_loader = create_dataloaders(X_trainB['custom_top_embeddings'], y_trainB, test_size=0.2, val_size=0.2, batch_size=64)\n",
        "binary_model = BinaryMLP(input_dim=len(X_trainB['custom_top_embeddings'].iloc[0]))\n",
        "trained_binary_model = train_model(binary_model, binary_train_loader, binary_val_loader, num_classes=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8TJrEXrxDoj",
        "outputId": "5ff941a1-5cba-49d4-fcd3-baea63e78357"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Custom top 10 features on Binary MLP: Metrics:\n",
            "accuracy, precision, recall, f1\n",
            "0.8869 0.9074 0.8618 0.8840\n"
          ]
        }
      ],
      "source": [
        "test_true, test_pred = get_predictions(trained_binary_model, binary_test_loader)\n",
        "evaluate(test_true, test_pred, \"Custom top 10 features on Binary MLP:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wcaZ7dDxDoj"
      },
      "source": [
        "#### Ternary model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvMoB9hdxDoj",
        "outputId": "bc29b805-6159-4e49-f5a1-ef36c9822181"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "Train Loss: 0.7635 | Val Loss: 0.6308 | Val Acc: 0.7286\n",
            "Epoch 2/20\n",
            "Train Loss: 0.6992 | Val Loss: 0.6097 | Val Acc: 0.7400\n",
            "Epoch 3/20\n",
            "Train Loss: 0.6809 | Val Loss: 0.6086 | Val Acc: 0.7427\n",
            "Epoch 4/20\n",
            "Train Loss: 0.6708 | Val Loss: 0.6070 | Val Acc: 0.7435\n",
            "Epoch 5/20\n",
            "Train Loss: 0.6633 | Val Loss: 0.6021 | Val Acc: 0.7444\n",
            "Epoch 6/20\n",
            "Train Loss: 0.6579 | Val Loss: 0.6013 | Val Acc: 0.7455\n",
            "Epoch 7/20\n",
            "Train Loss: 0.6554 | Val Loss: 0.5987 | Val Acc: 0.7461\n",
            "Epoch 8/20\n",
            "Train Loss: 0.6499 | Val Loss: 0.6009 | Val Acc: 0.7414\n",
            "Epoch 9/20\n",
            "Train Loss: 0.6467 | Val Loss: 0.6018 | Val Acc: 0.7446\n",
            "Epoch 10/20\n",
            "Train Loss: 0.6440 | Val Loss: 0.5983 | Val Acc: 0.7460\n",
            "Epoch 11/20\n",
            "Train Loss: 0.6419 | Val Loss: 0.6009 | Val Acc: 0.7460\n",
            "Epoch 12/20\n",
            "Train Loss: 0.6408 | Val Loss: 0.5977 | Val Acc: 0.7456\n",
            "Epoch 13/20\n",
            "Train Loss: 0.6387 | Val Loss: 0.6014 | Val Acc: 0.7458\n",
            "Epoch 14/20\n",
            "Train Loss: 0.6361 | Val Loss: 0.5972 | Val Acc: 0.7486\n",
            "Epoch 15/20\n",
            "Train Loss: 0.6320 | Val Loss: 0.5976 | Val Acc: 0.7479\n",
            "Epoch 16/20\n",
            "Train Loss: 0.6356 | Val Loss: 0.5995 | Val Acc: 0.7475\n",
            "Epoch 17/20\n",
            "Train Loss: 0.6330 | Val Loss: 0.6012 | Val Acc: 0.7480\n",
            "Early stopping!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Aditya Roy\\AppData\\Local\\Temp\\ipykernel_20652\\3495807950.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('best_model.pth'))\n"
          ]
        }
      ],
      "source": [
        "ternary_train_loader, ternary_val_loader, ternary_test_loader = create_dataloaders(X_trainT['custom_top_embeddings'], y_trainT, test_size=0.2, val_size=0.2, batch_size=64)\n",
        "ternary_model = TernaryMLP(input_dim=len(X_trainT['custom_top_embeddings'].iloc[0]))\n",
        "trained_ternary_model = train_model(ternary_model, ternary_train_loader, ternary_val_loader, num_classes=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEuEKKhrxDoj",
        "outputId": "5b9dd8c4-dca5-4354-ea49-8f96300b3c0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Custom top 10 features on ternary MLP: Metrics:\n",
            "accuracy, precision, recall, f1\n",
            "0.7439 0.7460 0.6602 0.6567\n"
          ]
        }
      ],
      "source": [
        "test_true, test_pred = get_predictions(trained_ternary_model, ternary_test_loader)\n",
        "evaluate_multi(test_true, test_pred, \"Custom top 10 features on ternary MLP:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRu-8WgvxDok"
      },
      "source": [
        "## Model Comparisons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fH0VU18gxDok",
        "outputId": "31b67b2d-4a2b-4e4d-87de-e1cef0468cd9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_name</th>\n",
              "      <th>Pretrained_embeddings</th>\n",
              "      <th>Custom_embeddings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Perceptron</td>\n",
              "      <td>0.8458</td>\n",
              "      <td>0.8548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SVM</td>\n",
              "      <td>0.8657</td>\n",
              "      <td>0.8985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Average embedding MLP</td>\n",
              "      <td>0.8792</td>\n",
              "      <td>0.9061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Top 10 embedding MLP</td>\n",
              "      <td>0.8809</td>\n",
              "      <td>0.8869</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              model_name  Pretrained_embeddings  Custom_embeddings\n",
              "0             Perceptron                 0.8458             0.8548\n",
              "1                    SVM                 0.8657             0.8985\n",
              "2  Average embedding MLP                 0.8792             0.9061\n",
              "3   Top 10 embedding MLP                 0.8809             0.8869"
            ]
          },
          "execution_count": 117,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "acc = {\n",
        "    \"model_name\":[\"Perceptron\",\"SVM\",\"Average embedding MLP\",\"Top 10 embedding MLP\"],\n",
        "    \"Pretrained_embeddings\":[0.8458,0.8657,0.8792,0.8809],\n",
        "    \"Custom_embeddings\":[0.8548,0.8985,0.9061,0.8869]\n",
        "}\n",
        "acc = pd.DataFrame(acc)\n",
        "acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJzJWYboxDok"
      },
      "source": [
        "Custom Embedding Superiority: Custom embeddings consistently outperform pretrained across all models, reinforcing their domain-specific advantage.\n",
        "\n",
        "MLP Effectiveness: Neural architectures (Average MLP: 90.61%) surpass traditional models (SVM: 89.85%) with custom embeddings, suggesting MLPs better leverage distributed semantic representations.\n",
        "\n",
        "Aggregation Impact: Average embeddings slightly outperforms Top-10 embeddings, implying comprehensive sequence modeling beats selective token focus for this task.\n",
        "\n",
        "Model Hierarchy: Performance ascends as Perceptron < SVM < Top-10 MLP < Average MLP, highlighting architectural complexity benefits when paired with quality embeddings.\n",
        "\n",
        "While custom embeddings universally boost performance, gains scale with model sophisticationMLPs extract 2-3% more accuracy from embeddings than linear models, emphasizing the value of pairing domain-adapted features with sufficiently expressive architectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1vJ1Eu8xDok"
      },
      "source": [
        "# Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StS1oQy6xDok"
      },
      "outputs": [],
      "source": [
        "class BinaryCNN(nn.Module):\n",
        "    def __init__(self, embedding_dim, sequence_length=50):\n",
        "        super(BinaryCNN, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv1d(1, 50, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(50, 10, kernel_size=3, padding=1)\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(10, 1)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x.unsqueeze(1)\n",
        "\n",
        "        # Convolutional layers\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "\n",
        "        # Global max pooling\n",
        "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
        "\n",
        "        # Dropout and output\n",
        "        x = self.dropout(x)\n",
        "        return self.fc(x).squeeze()\n",
        "\n",
        "class TernaryCNN(nn.Module):\n",
        "    def __init__(self, embedding_dim, sequence_length=50):\n",
        "        super(TernaryCNN, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv1d(1, 50, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(50, 10, kernel_size=3, padding=1)\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Linear(10, 3)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = x.unsqueeze(1)\n",
        "\n",
        "        # Convolutional layers\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "\n",
        "        # Global max pooling\n",
        "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
        "\n",
        "        # Dropout and output\n",
        "        x = self.dropout(x)\n",
        "        return self.fc(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PY1PWYyAxDol"
      },
      "outputs": [],
      "source": [
        "def train_cnn(model, train_loader, val_loader, num_classes=2, num_epochs=15, learning_rate=0.001):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Define loss and optimizer\n",
        "    criterion = nn.BCEWithLogitsLoss() if num_classes == 2 else nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    patience = 3\n",
        "    trigger_times = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            if num_classes == 2:\n",
        "                loss = criterion(outputs, labels.float())\n",
        "            else:\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        train_loss /= len(train_loader.dataset)\n",
        "\n",
        "        # Validation\n",
        "        val_loss, val_acc = evaluate_cnn(model, val_loader, num_classes, device)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}')\n",
        "        print(f'Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}')\n",
        "\n",
        "        # Early stopping\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            trigger_times = 0\n",
        "            torch.save(model.state_dict(), 'best_cnn_model.pth')\n",
        "        else:\n",
        "            trigger_times += 1\n",
        "            if trigger_times >= patience:\n",
        "                print(\"Early stopping!\")\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(torch.load('best_cnn_model.pth'))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8GWwQS7xDol"
      },
      "outputs": [],
      "source": [
        "def evaluate_cnn(model, data_loader, num_classes, device='cpu'):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    criterion = nn.BCEWithLogitsLoss() if num_classes == 2 else nn.CrossEntropyLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            if num_classes == 2:\n",
        "                loss = criterion(outputs, labels.float())\n",
        "                preds = (torch.sigmoid(outputs) > 0.5).long()\n",
        "            else:\n",
        "                loss = criterion(outputs, labels)\n",
        "                preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / total\n",
        "    accuracy = correct / total\n",
        "    return avg_loss, accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E04PsLETxDol"
      },
      "outputs": [],
      "source": [
        "def get_predictions(model, dataloader, device='cpu'):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            preds = (torch.sigmoid(outputs) > 0.5).cpu().numpy()\n",
        "\n",
        "            all_preds.extend(preds)\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    return np.array(all_labels), np.array(all_preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lgkx_-PoxDol"
      },
      "source": [
        "## Pretrained Word2Vec features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbWbr8AFxDol"
      },
      "source": [
        "### Binary model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cijxK1ruxDom",
        "outputId": "8c700302-ff8e-4357-f73a-eb5276de450a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "Train Loss: 0.6936 | Val Loss: 0.6925 | Val Acc: 0.5221\n",
            "Epoch 2/20\n",
            "Train Loss: 0.6900 | Val Loss: 0.6852 | Val Acc: 0.5451\n",
            "Epoch 3/20\n",
            "Train Loss: 0.6874 | Val Loss: 0.6855 | Val Acc: 0.5447\n",
            "Epoch 4/20\n",
            "Train Loss: 0.6877 | Val Loss: 0.6844 | Val Acc: 0.5477\n",
            "Epoch 5/20\n",
            "Train Loss: 0.6853 | Val Loss: 0.6832 | Val Acc: 0.5428\n",
            "Epoch 6/20\n",
            "Train Loss: 0.6840 | Val Loss: 0.6803 | Val Acc: 0.5514\n",
            "Epoch 7/20\n",
            "Train Loss: 0.6836 | Val Loss: 0.6804 | Val Acc: 0.5507\n",
            "Epoch 8/20\n",
            "Train Loss: 0.6836 | Val Loss: 0.6784 | Val Acc: 0.5568\n",
            "Epoch 9/20\n",
            "Train Loss: 0.6819 | Val Loss: 0.6763 | Val Acc: 0.5747\n",
            "Epoch 10/20\n",
            "Train Loss: 0.6792 | Val Loss: 0.6716 | Val Acc: 0.5856\n",
            "Epoch 11/20\n",
            "Train Loss: 0.6726 | Val Loss: 0.6614 | Val Acc: 0.6157\n",
            "Epoch 12/20\n",
            "Train Loss: 0.6690 | Val Loss: 0.6584 | Val Acc: 0.6158\n",
            "Epoch 13/20\n",
            "Train Loss: 0.6668 | Val Loss: 0.6547 | Val Acc: 0.6241\n",
            "Epoch 14/20\n",
            "Train Loss: 0.6660 | Val Loss: 0.6576 | Val Acc: 0.6116\n",
            "Epoch 15/20\n",
            "Train Loss: 0.6657 | Val Loss: 0.6535 | Val Acc: 0.6271\n",
            "Epoch 16/20\n",
            "Train Loss: 0.6653 | Val Loss: 0.6543 | Val Acc: 0.6234\n",
            "Epoch 17/20\n",
            "Train Loss: 0.6655 | Val Loss: 0.6514 | Val Acc: 0.6280\n",
            "Epoch 18/20\n",
            "Train Loss: 0.6645 | Val Loss: 0.6530 | Val Acc: 0.6250\n",
            "Epoch 19/20\n",
            "Train Loss: 0.6648 | Val Loss: 0.6526 | Val Acc: 0.6210\n",
            "Epoch 20/20\n",
            "Train Loss: 0.6639 | Val Loss: 0.6539 | Val Acc: 0.6175\n",
            "Early stopping!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Aditya Roy\\AppData\\Local\\Temp\\ipykernel_10004\\3495807950.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('best_model.pth'))\n"
          ]
        }
      ],
      "source": [
        "binary_train_loader, binary_val_loader, binary_test_loader = create_dataloaders(X_trainB[\"pretrained_average_50_embeddings\"], y_trainB, test_size=0.2, val_size=0.2, batch_size=64)\n",
        "binary_model = BinaryCNN(embedding_dim=len(X_trainB['pretrained_average_50_embeddings'].iloc[0]))\n",
        "trained_binary_model = train_model(binary_model, binary_train_loader, binary_val_loader, num_classes=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eddG2wBqxDom",
        "outputId": "1490d7ae-3a1b-493a-d9dc-4931417854b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pretrained average of 50 features on Binary CNN: Metrics:\n",
            "accuracy, precision, recall, f1\n",
            "0.6322 0.6797 0.4998 0.5760\n"
          ]
        }
      ],
      "source": [
        "test_true, test_pred = get_predictions(trained_binary_model, binary_test_loader)\n",
        "evaluate(test_true, test_pred, \"Pretrained average of 50 features on Binary CNN:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEI0Bvt2xDom"
      },
      "source": [
        "### Ternary model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xQFDiNcxDom",
        "outputId": "c5a44f54-89ae-4fb1-f7b7-7297124610c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "Train Loss: 1.0587 | Val Loss: 1.0516 | Val Acc: 0.4542\n",
            "Epoch 2/20\n",
            "Train Loss: 1.0482 | Val Loss: 1.0395 | Val Acc: 0.4683\n",
            "Epoch 3/20\n",
            "Train Loss: 1.0399 | Val Loss: 1.0317 | Val Acc: 0.4817\n",
            "Epoch 4/20\n",
            "Train Loss: 1.0368 | Val Loss: 1.0293 | Val Acc: 0.4875\n",
            "Epoch 5/20\n",
            "Train Loss: 1.0352 | Val Loss: 1.0296 | Val Acc: 0.4817\n",
            "Epoch 6/20\n",
            "Train Loss: 1.0337 | Val Loss: 1.0263 | Val Acc: 0.4857\n",
            "Epoch 7/20\n",
            "Train Loss: 1.0334 | Val Loss: 1.0238 | Val Acc: 0.4948\n",
            "Epoch 8/20\n",
            "Train Loss: 1.0318 | Val Loss: 1.0262 | Val Acc: 0.4853\n",
            "Epoch 9/20\n",
            "Train Loss: 1.0308 | Val Loss: 1.0233 | Val Acc: 0.4955\n",
            "Epoch 10/20\n",
            "Train Loss: 1.0309 | Val Loss: 1.0211 | Val Acc: 0.4974\n",
            "Epoch 11/20\n",
            "Train Loss: 1.0303 | Val Loss: 1.0189 | Val Acc: 0.5038\n",
            "Epoch 12/20\n",
            "Train Loss: 1.0293 | Val Loss: 1.0166 | Val Acc: 0.5061\n",
            "Epoch 13/20\n",
            "Train Loss: 1.0283 | Val Loss: 1.0180 | Val Acc: 0.5034\n",
            "Epoch 14/20\n",
            "Train Loss: 1.0285 | Val Loss: 1.0155 | Val Acc: 0.5064\n",
            "Epoch 15/20\n",
            "Train Loss: 1.0265 | Val Loss: 1.0145 | Val Acc: 0.5083\n",
            "Epoch 16/20\n",
            "Train Loss: 1.0266 | Val Loss: 1.0168 | Val Acc: 0.5073\n",
            "Epoch 17/20\n",
            "Train Loss: 1.0256 | Val Loss: 1.0135 | Val Acc: 0.5089\n",
            "Epoch 18/20\n",
            "Train Loss: 1.0254 | Val Loss: 1.0200 | Val Acc: 0.4948\n",
            "Epoch 19/20\n",
            "Train Loss: 1.0213 | Val Loss: 1.0090 | Val Acc: 0.5145\n",
            "Epoch 20/20\n",
            "Train Loss: 1.0199 | Val Loss: 1.0095 | Val Acc: 0.5125\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Aditya Roy\\AppData\\Local\\Temp\\ipykernel_10004\\3495807950.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('best_model.pth'))\n"
          ]
        }
      ],
      "source": [
        "ternary_train_loader, ternary_val_loader, ternary_test_loader = create_dataloaders(X_trainT['pretrained_average_50_embeddings'], y_trainT, test_size=0.2, val_size=0.2, batch_size=64)\n",
        "ternary_model = TernaryCNN(embedding_dim=len(X_trainT['pretrained_average_50_embeddings'].iloc[0]))\n",
        "trained_ternary_model = train_model(ternary_model, ternary_train_loader, ternary_val_loader, num_classes=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtRwRYV6xDom",
        "outputId": "83ae6ff4-12ed-4ab5-fc02-9699be113e70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ternary CNN on pretrained embeddings accuracy:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.511425"
            ]
          },
          "execution_count": 153,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"ternary CNN on pretrained embeddings accuracy:\")\n",
        "evaluate_cnn(ternary_model, ternary_test_loader, 3)[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmhkcf3txDom"
      },
      "source": [
        "## Custom Word2Vec features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nezf7XzCxDon"
      },
      "source": [
        "### Binary model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRngFoutxDon",
        "outputId": "8b246071-3998-4761-c316-7f4b9e097c84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "Train Loss: 0.6938 | Val Loss: 0.6927 | Val Acc: 0.5725\n",
            "Epoch 2/20\n",
            "Train Loss: 0.6887 | Val Loss: 0.6762 | Val Acc: 0.6108\n",
            "Epoch 3/20\n",
            "Train Loss: 0.6776 | Val Loss: 0.6693 | Val Acc: 0.6237\n",
            "Epoch 4/20\n",
            "Train Loss: 0.6737 | Val Loss: 0.6631 | Val Acc: 0.6320\n",
            "Epoch 5/20\n",
            "Train Loss: 0.6716 | Val Loss: 0.6586 | Val Acc: 0.6272\n",
            "Epoch 6/20\n",
            "Train Loss: 0.6675 | Val Loss: 0.6480 | Val Acc: 0.6456\n",
            "Epoch 7/20\n",
            "Train Loss: 0.6662 | Val Loss: 0.6509 | Val Acc: 0.6395\n",
            "Epoch 8/20\n",
            "Train Loss: 0.6632 | Val Loss: 0.6462 | Val Acc: 0.6474\n",
            "Epoch 9/20\n",
            "Train Loss: 0.6652 | Val Loss: 0.6522 | Val Acc: 0.6388\n",
            "Epoch 10/20\n",
            "Train Loss: 0.6640 | Val Loss: 0.6447 | Val Acc: 0.6518\n",
            "Epoch 11/20\n",
            "Train Loss: 0.6612 | Val Loss: 0.6463 | Val Acc: 0.6516\n",
            "Epoch 12/20\n",
            "Train Loss: 0.6602 | Val Loss: 0.6469 | Val Acc: 0.6487\n",
            "Epoch 13/20\n",
            "Train Loss: 0.6585 | Val Loss: 0.6390 | Val Acc: 0.6567\n",
            "Epoch 14/20\n",
            "Train Loss: 0.6530 | Val Loss: 0.6329 | Val Acc: 0.6616\n",
            "Epoch 15/20\n",
            "Train Loss: 0.6499 | Val Loss: 0.6303 | Val Acc: 0.6584\n",
            "Epoch 16/20\n",
            "Train Loss: 0.6482 | Val Loss: 0.6310 | Val Acc: 0.6624\n",
            "Epoch 17/20\n",
            "Train Loss: 0.6503 | Val Loss: 0.6292 | Val Acc: 0.6635\n",
            "Epoch 18/20\n",
            "Train Loss: 0.6478 | Val Loss: 0.6251 | Val Acc: 0.6667\n",
            "Epoch 19/20\n",
            "Train Loss: 0.6483 | Val Loss: 0.6244 | Val Acc: 0.6682\n",
            "Epoch 20/20\n",
            "Train Loss: 0.6449 | Val Loss: 0.6257 | Val Acc: 0.6613\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Aditya Roy\\AppData\\Local\\Temp\\ipykernel_10004\\3495807950.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('best_model.pth'))\n"
          ]
        }
      ],
      "source": [
        "\n",
        "binary_train_loader, binary_val_loader, binary_test_loader = create_dataloaders(X_trainB['pretrained_average_50_embeddings'], y_trainB, test_size=0.2, val_size=0.2, batch_size=64)\n",
        "binary_model = BinaryCNN(embedding_dim=len(X_trainB['pretrained_average_50_embeddings'].iloc[0]))\n",
        "trained_binary_model = train_model(binary_model, binary_train_loader, binary_val_loader, num_classes=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zYoqKznxDon",
        "outputId": "41638d06-79e1-49ec-ae9f-cf46e7f3dd54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Custom average of 50 features on ternary CNN: Metrics:\n",
            "accuracy, precision, recall, f1\n",
            "0.6640 0.6561 0.6891 0.6722\n"
          ]
        }
      ],
      "source": [
        "test_true, test_pred = get_predictions(trained_binary_model, binary_test_loader)\n",
        "evaluate(test_true, test_pred, \"Custom average of 50 features on ternary CNN:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amBjB4tXxDon"
      },
      "source": [
        "### Ternary model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AV0suqEExDon",
        "outputId": "c8cc3882-7e50-45a0-a13e-b9c4dd8a5405"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "Train Loss: 1.0587 | Val Loss: 1.0521 | Val Acc: 0.4835\n",
            "Epoch 2/20\n",
            "Train Loss: 1.0450 | Val Loss: 1.0327 | Val Acc: 0.4892\n",
            "Epoch 3/20\n",
            "Train Loss: 1.0385 | Val Loss: 1.0311 | Val Acc: 0.4809\n",
            "Epoch 4/20\n",
            "Train Loss: 1.0355 | Val Loss: 1.0228 | Val Acc: 0.5004\n",
            "Epoch 5/20\n",
            "Train Loss: 1.0310 | Val Loss: 1.0197 | Val Acc: 0.4996\n",
            "Epoch 6/20\n",
            "Train Loss: 1.0289 | Val Loss: 1.0138 | Val Acc: 0.5107\n",
            "Epoch 7/20\n",
            "Train Loss: 1.0279 | Val Loss: 1.0125 | Val Acc: 0.5126\n",
            "Epoch 8/20\n",
            "Train Loss: 1.0263 | Val Loss: 1.0120 | Val Acc: 0.5128\n",
            "Epoch 9/20\n",
            "Train Loss: 1.0262 | Val Loss: 1.0171 | Val Acc: 0.5064\n",
            "Epoch 10/20\n",
            "Train Loss: 1.0254 | Val Loss: 1.0089 | Val Acc: 0.5147\n",
            "Epoch 11/20\n",
            "Train Loss: 1.0230 | Val Loss: 1.0090 | Val Acc: 0.5130\n",
            "Epoch 12/20\n",
            "Train Loss: 1.0214 | Val Loss: 1.0051 | Val Acc: 0.5194\n",
            "Epoch 13/20\n",
            "Train Loss: 1.0198 | Val Loss: 1.0046 | Val Acc: 0.5164\n",
            "Epoch 14/20\n",
            "Train Loss: 1.0177 | Val Loss: 1.0026 | Val Acc: 0.5205\n",
            "Epoch 15/20\n",
            "Train Loss: 1.0168 | Val Loss: 0.9988 | Val Acc: 0.5232\n",
            "Epoch 16/20\n",
            "Train Loss: 1.0139 | Val Loss: 0.9955 | Val Acc: 0.5272\n",
            "Epoch 17/20\n",
            "Train Loss: 1.0137 | Val Loss: 0.9973 | Val Acc: 0.5264\n",
            "Epoch 18/20\n",
            "Train Loss: 1.0141 | Val Loss: 0.9945 | Val Acc: 0.5290\n",
            "Epoch 19/20\n",
            "Train Loss: 1.0113 | Val Loss: 0.9946 | Val Acc: 0.5266\n",
            "Epoch 20/20\n",
            "Train Loss: 1.0111 | Val Loss: 0.9963 | Val Acc: 0.5253\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Aditya Roy\\AppData\\Local\\Temp\\ipykernel_10004\\3495807950.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('best_model.pth'))\n"
          ]
        }
      ],
      "source": [
        "\n",
        "ternary_train_loader, ternary_val_loader, ternary_test_loader = create_dataloaders(X_trainT['pretrained_average_50_embeddings'], y_trainT, test_size=0.2, val_size=0.2, batch_size=64)\n",
        "ternary_model = TernaryCNN(embedding_dim=len(X_trainT['pretrained_average_50_embeddings'].iloc[0]))\n",
        "trained_ternary_model = train_model(ternary_model, ternary_train_loader, ternary_val_loader, num_classes=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkXk6FJuxDon",
        "outputId": "856d6624-00f7-4e18-e5db-9b35bbc4c161"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ternary CNN on pretrained embeddings accuracy:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.5253"
            ]
          },
          "execution_count": 158,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# test_true, test_pred = get_predictions(trained_ternary_model, ternary_test_loader)\n",
        "print(\"ternary CNN on pretrained embeddings accuracy:\")\n",
        "evaluate_cnn(ternary_model, ternary_test_loader, 3)[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ld1nU9NixDoo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AKv1SmPxDoo"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}